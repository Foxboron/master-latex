% !TEX encoding = UTF-8 Unicode
%!TEX root = ../Main/thesis.tex
% !TEX spellcheck = en-US
%%=========================================
\documentclass[../Main/thesis.tex]{subfiles}
\begin{document}
\chapter{Theory}
\label{ch:theory}
In this section we will take a closer look at the theory surrounding
supply chains, reproducible builds, transparency logs and rebuilders.


\section{Supply Chains}\label{sec:supply_chain}
\subsection*{The Supply Chain}
Most of the software today are developed through a series of steps. This is
traditionally called ``The Software Supply Chain''. Software projects today go
through development, building, testing, staging and in the end production, with
slight variations between them. This is largely done with self-hosted solutions,
or outsourced to external hosting solution.

In the world of open-source the software delivery are usually done by Linux
distributions, or other similar methods of distributions. The supply chain in
this regard is the complete steps from developers getting the source code for
the project, until it is delivered as a compiled artifact to the end-user. This
also includes the wider network of packages that are needed for the distribution
of the project. To get understand the wider problem of delivering secure
software, we will contrast ``Software Supply Chain'' with the more commonly
thought of ``Software Delivery Lifecycle'' to understand what the supply chain
encompasses.

\citeauthor{10.1109CSAC.2004.41} in~\citetitle{10.1109CSAC.2004.41} outlines the
``Software Delivery Lifecycle'' as a development model on how to deliver secure
software~\cite{10.1109CSAC.2004.41}. This is done with the following steps;

\begin{itemize}
    \item Requirements
    \item Design
    \item Implementation
    \item Verification 
    \item Release
    \item Deployment
    \item Response
\end{itemize}

For each of these steps there are adequate security measures assigned. The
``Requirements'' step would need the developers to assess the security
requirements of the process, and to make sure any milestones are met. The
``Release'' step would for instance include a penetration testing, where an
active party attempts to hack or compromise the given software, and have a
threat model reviewed where the security concerns are addressed and in some case
justified.

This model only encompasses the development and the code written by the authors
and is fairly similar to a traditional software models when the security aspects
of it is removed. However, we are lacking a few considerations from this model;
distributions and the wider ecosystem that is involved writing and producing
software. \citeauthor{rj-ellison-2010} in \citetitle{rj-ellison-2010} they
analyze how the United states Department of Defense handles software acquisition
in a very high secure environment. 


In the context of the military ``[\dots] supply chains typically
involve the movement of materials from home base to troops in theater. The
responsibility for managing these supply chains falls to the acquisition and
logistics experts''~\cite{rj-ellison-2010}. In this case, the DoD is not
producing any software. Their only concern is to get the software developed,
tested, shipped and updated in a secure fashion. In all of these steps they
might relay on outside contractors and thus have to safeguard themselves from
any risks. This in turns makes the supply chain far larger, and far more
encompassing then a development model like the software lifecycle looked upon
earlier.

In this thesis we will focus on how package managers work as a supply chain for
distributing software in a secure manner.


\subsubsection*{in-toto}
in-toto is a framework to verify the integrity of a supply chain. It defines a
specification that details what steps should occur. As one supply chain could
define and utilize any number of steps its vital for this to be extensible, and
customize able. In-toto lets the specification detail who should perform the
step in the supply chain.

The layout describes what each step of the supply chain should contain. It can
be any expected commands, something the process should succeed running, any
expected material, things needed for the step in the chain to proceed, and any
products, artifacts created by the steps. These are described in a very small
language that contains keywords following regular expressions which should be
satisfied.

The link metadata is a JSON file that specifies what the values, and outputs of
the corresponding step should be. Evaluating the specification along with the
link metadata lets the users, or the organization, verify that the supply chain
has not been tampered with. 

\begin{listing}[H]
\begin{minted}[]{json}
{"signatures": [],
 "signed": {
  "_type": "layout",
  "expires": "2021-01-06T18:30:57Z",
  "inspect": [{
        "_type": "inspection",
        "expected_materials": [
             ["MATCH", "*.deb", "WITH", "PRODUCTS", "FROM", "rebuild"],
             ["DISALLOW", "*.deb"]],
        "expected_products": [],
        "name": "verify-reprobuilds",
        "run": ["/usr/bin/true"]}],
  "keys": {
        "2e7be98291270e3b7fca429a2210e99cff22017e":{
             "hashes": ["pgp+SHA2"],
             "keyid": "2e7be98291270e3b7fca429a2210e99cff22017e",
             "keyval": {"private": "", "public": {"e": "010001", "n": "e0da84bec..."}},
             "method": "pgp+rsa-pkcsv1.5",
             "type": "rsa"}},
  "readme": "",
  "steps": [{
        "_type": "step",
        "expected_products": [
             ["CREATE", "*.deb"],
             ["DISALLOW", "*.deb"]],
        "name": "rebuild",
        "pubkeys": ["2e7be98291270e3b7fca429a2210e99cff22017e"],
        "threshold": 1}]
}}
\end{minted}
\caption{Example in-toto schema}
\label{lst:in-toto-schema}
\end{listing}

\begin{listing}[H]
\begin{minted}[]{json}
{"signatures": [
  {"keyid": "918b19596...",
   "other_headers": "0400010800...",
   "signature": "bc1d9776bf..."}],
 "signed": {
  "_type": "link",
  "name": "rebuild",
  "products": {
   "python-sshpubkeys_3.1.0-1_all.deb": {"sha256": "8e69d5cbdc..."},
   "python3-sshpubkeys_3.1.0-1_all.deb": {"sha256": "8234484139..."}}
}}
\end{minted}
\caption{Example linkmetadata file}
\label{lst:linkmetadata}
\end{listing}

\section{Linux Distributions}\label{sec:linux_distributions}
\subsection*{Linux}
Linux is free and open-source kernel. It was first developed by Linus Torvalds
in the early 1990 and has grown into the largest open-source project today. It
is commonly used in everything from firmware modules on a computer, to the every
increasing field of Internet of Thing, along with servers and on personal
computers. The development of Linux is distributed and has spawned the
``Open-Source'' method of developing software.

Linux is accompanied by a suit of tools and environment that is commonly
referred to as a ``distribution'' and defines an operating system based on
Linux. These are created by companies as commercial products, as well as groups
of volunteers as a hobby for free. The tooling of these distributions, along
with organization and the inherent supply chain to deliver artifacts to the
users, are unique to each project. Some are ``source''-distributions, and only
distribute build recipes, and some distributed pre-compiled binary packages.


\subsection*{Debian}
Debian was one of the first operating systems based on Linux, and was created by
Ian Murdock in 1993. One of the main innovations from Debian was the creation of
the very first package manager. Package manager allows users to download
pre-compiled software from centralized repositories maintained by the Debian
developers. This allows users to easily fetch, update and remove installed
packages on their system.

These packages are maintained by package maintainers who package, update and
maintain the required files to distribute this to the end user. Each maintainer
has a cryptographic secure singing key they use to fetch, and publish source
packages to a build server. These source packages contains the needed files,
patches and package files to compile the project source code to a format
accepted by the Debian package manager, apt.

The build server verifies the signatures and compile these source packages to
the all the supported architectures. It will then sign these packages with its
own key, and then distribute these to the end-user in the form of a mirror
system.


\section{Software determinism}\label{sec:reproducible_builds}
% TODO: Usikker p√• struktur.
% Skal seksjonen hete reproducible builds? 

% TODO: En seksjon ang "trust"?

\subsection*{Trusting trust}
In a paper from Ken Thompson in 1984, after he won the Turing Award for his work
on the UNIX operating system, \citetitle{ken-thompson-1984}, Thompson as a
programming exercise implements a very basic self-reproducing program. As a
demonstration, he adds code capable of introducing new code when certain
patterns are encountered. This implements a backdoor in something that could

\begin{quotation}
``You can't trust code that you did not totally create yourself. [...] No amount
of source-level verification or scrutiny will protect you from using untrusted
code. In demonstrating the possibility of this kind of attack, I picked on the C
compiler. I could have picked on any program-handling program such as an
assembler, a loader, or even hardware microcode. As the level of program gets
lower, these bugs will be harder and harder to detect.  A well-installed
microcode bug will be almost impossible to detect''\cite{ken-thompson-1984}.
\end{quotation}

% TODO: Research into software verifiability
This paints a very bleak picture, considering most software we get today is
pre-compiled and provided to us by different vendors.

% TODO: Move into diverse compilation. Reprobuilds an extension

But there are possible ways to counter this problem.  David A. Wheeler in his
dissertation \citetitle{Wheeler:2005:CTT:1106778.1106809} details a possible solution to
the ``trusting trust''-problem. It involves what he call diverse ``diverse
compilation'' \cite{Wheeler:2005:CTT:1106778.1106809}.

``Diverse double-compilation'', or ``DCC'', is the act of using two compilers to
detect any difference in the resulting artifact. The first compilation is done
with a secondary compiler, then again with the primary compiler. The idea is
that the secondary compiler is a minimal implementation of the compiler, and can
be trusted.

% TODO: Skrive mer

\subsection*{Reproducible builds}
Reproducible builds is a set of practices for how to achieve deterministic
compilation of software. Supply chains are usually handled with multiple tools,
and on several individual computers, this leaves a rather large attack surface
for malicious actors to try compromise the chain. This is not a theoretical
threat. There has been an increase in attacks on parts of supply chains in
recent years. They are high impact and affect users as well as developers.

According to the definition of reproducible builds;

\begin{quotation}
``A build is reproducible if given the same source code, build environment
and build instructions, any party can recreate bit-by-bit identical copies of
all specified artifacts''~\cite{reproducible-builds-2019-definitions}.
\end{quotation}

One of the earliest open-source projects to promote reproducible builds is the
Tor project. The Tor project develops the ``Tor network'' which is anonymity
network compromised of volunteer that run network nodes that effectively
anonymize the network of the user~\cite{tor}. This is heavily used by dissidents
in oppressive regimes where the internet connection is filtered, or partially
blocked. To help giving access to this network, they develop a variant of the
web browser firefox called ``Tor browser''~\cite{tor-browser}.

% TODO: Add sources

This browser is configured to utilize the tor network for an anonymous browsing
session. This has been a very important software used by marginalized
protesters. Because of this, the supply chain and distribution of this software
is important. Receiving a malicious version of this software could in many cases
result in prison of life threatening danger to people. Mike Perry highlighted
this concern when discussing the testing of the ``Tor browser''.

\begin{quotation}
``This means that software development has to evolve beyond the simple models of
"Trust my gpg-signed apt archive from my trusted build machine", or even
projects like Debian going to end up distributing state-sponsored malware in
short order''~\cite{mike-perry-2013}.
\end{quotation}

% TODO: Gitain

The result of this concern is the move to support reproducible builds. Allowing
users, and multiple independent builders, to recreate the distributed artifact
bit-for-bit ~\cite{unknown-2014}. This is done by utilizing ``Gitain'' which
builds, and packages the software on self-contained virtual machines. This
enables the project to distribute the same build instructions as used to package
the software in the first place, and allows users to easily verify if the
distributed artifact matches the self-produced one.

% TODO: Bitcoin

% TODO: Kilder
Around the same time, in 2013, Debian started a push towards reproducible
builds, and an effort into achieving this for their distributed packages. Since
then, 22 projects are officially part of the initiative and there has been 4
conferences has been held to tackle the problem.

\subsection*{Source Date Epoch}
One of the most common offenders for undeterministic builds is the embedding of
when something was built. On the surface this looks like a completely sane thing
to do for most builds, but this creates problems when the produced artifact in
turns become undeterministic because we built it at another point in time.

The reproducible builds project defines an environment variable called
``SOURCE\_DATE\_EPOCH'' which is an means to solve this dilemma
\cite{reproducible-builds-2019-source-date-epoch}. The variable enables software
distributors to build artifacts with an embedded time, but it also helps to
specify the time in a manner that enables reproducible builds as we can set the
date arbitrarily.

The requirement is that this variable is exported in the build system used to
create the package. It also needs to take the current date time if no
``SOURCE\_DATE\_EPOCH'' is provided.

\subsection*{Buildinfo}
% TODO: Kanskje nevne √∏kosystemer bedre?
One of the main issues with reproducible builds is that is hard to make
everything universally reproducible. Producing the same binary package on
multiple different linux distributions is close to impossible and unmanageable
for most software. Thus we need to specify the environment being utilized with
all of the requirements and idiosyncrasies.

This is not a new discovery.
\citeauthor{r.-torres-cabrera-and-bonnie-lee-appleton-1999} in their paper
\citetitle{r.-torres-cabrera-and-bonnie-lee-appleton-1999} from 1999, they
defined a ``Bill of Material'', or a ``BOM'' for short.

\begin{quotation}
``Document all of the components that contributed to the build in a list, i.e.,
a bill of materials (BOM). The BOM may contain the names, versions, and
directory paths of operating systems, libraries, compilers, linkers, make-files,
build scripts, etc The BOM may be manually created, but many configuration
management tools generate it as a by- product of the build'' \cite{r.-torres-cabrera-and-bonnie-lee-appleton-1999}.
\end{quotation}

\begin{listing}[H]
\begin{minted}[]{text}
Format: 1.0
Source: dh-make
Binary: dh-make
Architecture: all
Version: 2.201802
Checksums-Sha256:
22c95094efbe79445336007dd[...] 42360 dh-make_2.201802_all.deb
Build-Origin: Debian
Build-Architecture: amd64
Build-Kernel-Version: 4.9.0-8-amd64 #1 SMP Debian 4.9.110-3 (2018-10-08)
Build-Date: Thu, 06 Dec 2018 00:04:23 +0000
Build-Path: /build/dh-make-2.201802
Installed-Build-Depends:
autoconf (= 2.69-11),
automake (= 1:1.16.1-4),
[...]
xz-utils (= 5.2.2-1.3),
zlib1g (= 1:1.2.11.dfsg-1)
Environment:
DEB_BUILD_OPTIONS="buildinfo=+all reproducible=+all parallel=16"
LANG="C"
LC_ALL="POSIX"
SOURCE_DATE_EPOCH="1543231660"
\end{minted}
\caption{Example buildinfo file}
\label{lst:buildinfo}
\end{listing}

Such a file would contain all the requirements to recreate the environment the
artifact was built inn. Because of the previously mentioned restrictions, there
is a need for multiple formats to define the requirements. Currently there are
around 5 different formats for different ecosystems on the reproducible builds
website \cite{reproducible-builds-2019}.

% TODO: Legg til source p√• deb-buildinfo

The Debian format encompasses a wide array of values ``Format'' defines
the expected fields in the format and gets incremented with any changes.
``Source'' and ``Binary'' defines the source package used to produce this, and
the corresponding binary package it outputs. The ``Architecture'' fields defines
which architecture the product is compiled towards. Debian supports a wide array
of CPU architectures from ARM to AMD 64 bit. High-level languages usually does
not compile, therefor ``all'' is used to denote this.

The ``Build-'' variables denotes the build environment used to create the
artifact in the Debian ecosystem. Since Debian has a slew of derivative and
closely related distributions, ``Build-Origin'' is used to denote this
distribution.  ``Build-Architecture'' denotes the architecture of the build
server being used.  ``Build-Kernel-Version'' denotes the explicit  version of
the kernel used. This is commonly fetched with the command ``uname -a''.
``Build-Date'' refers to the ISO compatible date when the process took place.
``Build-Path'' is the location where the build took place.
``Installed-Build-Depends'' contains list of all packages present during the
packaging of this artefact. This s an important list to keep track of as it
enables the complete recreation of the environment at a later point.

Linux has several variables that sets the locale, language and timestamp format,
that can affect the build process. ``Environment'' encompasses all of these.
Most importantly the variable ``SOURCE\_DATE\_EPOCH'' is stored here to make sure
timestamps are deterministic.

% TODO: Lenke til buildinfo.debian.net
This file can then be distributed alongside the package, or provided through
other means. Debian archives all buildinfo-files on a centralized webpage where
they can be queried and retrieved.

% TODO: Add disorderfs source
% TODO: Add diffoscope source

\subsection*{disorderfs}
When distributing files it is very commonly to do so using file archives. Common
formats is the ZIP and the TAR format. However, the order in which files appear
in the archive needs to be consistent for the checksum to match, however this
can be hard to test in some cases. ``disorderfs'' is a filesystem that lets you
introduce unexpected and randomized behavior when reading files. This helps find
sources for non-deterministic artifacts in the build process.


\subsection*{diffoscope}
Diffoscope is a tool to help compare binary formats for differences. It supports
a lot of binary formats to help find reproducability issues in produced
artifacts. It enables the user to output reports of the comparison as plain text
files or HTML files so they can be easily embedded in webpages. The current CI
system in Debian provides the HTML files for easier debugging.

The software is packaged and used by a number of distributions and is currently
a very important tool in debugging reproducability issues.


\section{Rebuilders}\label{sec:rebuilders} 
One of the main ideals with reproducible builds is the ability to let the users
recreate distributed artifacts. This is achievable with the correct tooling, and
k ``BUILDINFO'' file as specified in the previous section. However, building all
distributed packages is an unwieldy task. The appeal of Linux distributions is
the ability to download pre-compiled packages to save the effort of building all
the software one intend to use.

% TODO: Add CI source

Currently Debian has put a great deal of effort into testing packages by setting
up a ``Continuous Integration'' framework for testing packages. This setup
compiles all Debian packages twice with variations to see if they end up
reproducible or not. This is a neat approach to find reproducability issues, but
it does not reproduce any packages produced and distributed by Debian developers
directly. They are merely built twice in their own environment. The real goal is
to reproduce distributed packages, so the CI solution it self does not fulfill
this goal.

What is needed is servers which takes packages as distributed, and reproduces
these. The general idea is that there should be a pool of diverse servers which
are capable of rebuilding packages in the correct environment. These rebuilders
should be capable of sharing some sort of attestation for the built package.

% TODO: Attestation sharing source
% TODO: Source - planned in 2014/2-16 by the project

\section{Merkle Trees}%
\label{sec:merkle_trees}

Merkle Trees is a tree structure based on cryptographic secure hashing function
\cite{ralph-c.-merkle-1998}. It creates a binary tree where each leaf is hashed,
and combined two and two. The top node of this tree is reffed to as a ''root
node''. The interesting property of merkle trees is the ability to verify the
inclusion of elements by calculating the path from the given leaf to the root
node. This can be done by acquiring the missing hashes for each intermediate
node, and then hash each of the steps together.

\subsubsection{Transparency logs}%
\label{sub:certificate_transparency_log}
Transparency logs is a use case of merkle-trees by~\citeauthor{182788} and
describes how merkle-trees can be used to achieve tamper evident
logging~\cite{182788}. Certificate transparency logs
from~\citeauthor{b.-laurie-a.-langley-e.kaster-google-2013}is an implementation
of transparency logs~\cite{b.-laurie-a.-langley-e.kaster-google-2013}.

Certificate transparency logs helps organizations issuing HTTPS certificates an
audit log of all certificates issued, and by whom. It allows the discoverability
of when abuse, and malicious issuing of certificates backwards in time, and also
allows organizations to see who issues certificates for what domains. This can
aid in detecting compromise early.

Proofs in the context of transparency logs are tuple pairs where the first
element describes the position, and the second element includes the hash of
the given object. Given the correct order of hashing, the product of this
should be some merkle tree root the log is either currently using, or have
used in the past.

The proofs needed to implement a transparency log is as follows;
\begin{itemize}
\item Audit proof
\item Consistency proof
\end{itemize}

\begin{figure}[H]
\centering
\subfloat[Merkle tree with leaf ``d5'' highlighted]{\begin{tikzpicture}
\Tree
[ .\node[root]{r};
[ .m   [ .i d1 d2 ]  [ .j d3 d4 ]  ]
[ .n  [ .k \node[proof]{d5}; d6 ]  [ .l d7 d8 ]  ]
]
\end{tikzpicture}}%
\qquad
\subfloat[needed roots for audit proof of ``d5'']{\begin{tikzpicture}
\Tree
[ .\node[root]{r};
[ .\node[roots]{m};   [ .i d1 d2 ]  [ .j d3 d4 ]  ]
[ .n  [ .k \node[proof]{d5}; \node[roots]{d6}; ]  [ .\node[roots]{l}; d7 d8 ]  ]
]
\end{tikzpicture}}%
\caption{Audit proof}
\label{fig:audit}
\end{figure}

Each of this proofs are needed to correctly verify that a transparency log
is correctly operating and can verify this to monitors.  Monitors follow
these logs to assure consistency and to make sure logs do not misbehave. It
also allows monitors to verify the append-only property of the log.

The audit proof is used to verify that the given element in the log exists
in the log. The tree root is given, along with the elements needed to
recreate the missing nodes for the root. In the Figure~\ref{fig:audit} we can
see the representation of a merkle tree. For the proof that the node $ d5 $ is
part of the merkle tree represented with the signed tree root $ r $ the needed
tree roots are $ r = \{ d5, d6, l, m \} $. If they are hashed together
appropriatly, such as $ r = H( m, H( H(d5, d6), l)) $, we will arrive at the
same value for and thus the proof is validated.

The consistency proof is used to verify that the log is operating as an
append-only log in a correct manner. This proof requires two things. A
previous merkle tree root, and the number of leafs present at the time of
this tree root. The returned path is the number of subroots needed to
recreate the path from the root, until the new root.

To have a correctly vetted tamper-evident log, there needs to be monitor server.

Monitor servers watch new entries from log servers and verify them. They
collect the current signed tree root, and makes sure they match up with the
currently fetched server. They are important to make sure that the log
collecting server behaves properly.

\subsection*{Transparency Log Overlays}%
\label{sub:transparency_log_overlays}
For our system we will add some semantic meaning to the leafs we add to the
merkle tree. This will be explained in greater details in the development of
this in Section~\ref{sub:fourth_iteration_development} on
page~\pageref{sub:fourth_iteration_development}, however we will go into some of
the underlying concepts.

\citeauthor{10.11452976749.2978404} describes
in~\citetitle{10.11452976749.2978404} a concept of adding transparencies on top
of existing processes to achieve transparency and tamper-evident
events~\cite{10.11452976749.2978404}. They create a system greatly inspired by
the work of both~\citeauthor{182788}, in transparency logs~\cite{182788},
and~\citeauthor{b.-laurie-a.-langley-e.kaster-google-2013} in certificate
transparency logs~\cite{b.-laurie-a.-langley-e.kaster-google-2013}, and builds
on top of these concepts.

The idea of this approach is to log the everyday events of systems on a
transparency log to make sure the events are tamper-evident. The values of these
commitments on the log can be arbitrary and aid in providing some semantic
meaning to the underlying application. In our project, we will utilize this idea
to provide rebuild attestation, but also the ability to revoke such attestations
on the merkle tree.

\section*{Summary}\label{sec:summary-theory} 
In this chapter we have taken a deeper look at the theory surrounding supply
chains, reproducible builds and merkle trees. In the next chapter we will be
taking a look at the current research being done towards shared attestation on
reproducible builds and package transparency logs.
\blankpage
\end{document}
