% !TEX encoding = UTF-8 Unicode
%!TEX root = ../Main/thesis.tex
% !TEX spellcheck = en-US
%%=========================================
\documentclass[../Main/thesis.tex]{subfiles}
\begin{document}
\chapter{Theory}
\label{ch:theory}
In this section we will take a closer look at the theory surrounding
supply chains, reproducible builds, transparency logs and rebuilders.

\section{The Supply Chain}\label{sec:supply_chain}
Most of the software today are developed through a series of steps. This is
traditionally called ``The Software Supply Chain''. Software projects today go
through development, building, testing, staging and production, with slight
variations between them. This is largely done with self-hosted solutions, or
outsourced to external hosting solution.

In the world of open-source, the software delivery are usually done by Linux
distributions, or other similar methods of distributions. The supply chain in
this regard is the complete steps from developers getting the source code for
the project, until it is delivered as a compiled artifact to the end-user. This
also includes the wider network of packages that are needed for the distribution
of the project. To get understand the wider problem of delivering secure
software, we will contrast ``Software Supply Chain'' with the more commonly
thought of ``Software Delivery Lifecycle'' to understand what the supply chain
encompasses.

\citeauthor{10.1109CSAC.2004.41} in~\citetitle{10.1109CSAC.2004.41} outlines the
``Software Delivery Lifecycle'' as a development model on how to deliver secure
software~\cite{10.1109CSAC.2004.41}. This is done with the following steps;

\begin{itemize}
    \item Requirements
    \item Design
    \item Implementation
    \item Verification 
    \item Release
    \item Deployment
    \item Response
\end{itemize}

For each of these steps there are adequate security measures assigned. The
``Requirements'' step would need the developers to assess the security
requirements of the process, and to make sure any milestones are met. The
``Release'' step would for instance include a penetration testing, where an
active party attempts to hack or compromise the given software, and have a
threat model reviewed where the security concerns are addressed and in some case
justified.This model only encompasses the development and the code written by
the authors and is fairly similar to a traditional software models. 

Traditional software models like Agile development, waterfall and extreme
programming is all about managing the development lifecycle. Write code, respond
to changes and delivery a product. However, we are lacking a few considerations
from this model; distribution of the software and the wider ecosystem that is
involved writing and producing software. \citeauthor{rj-ellison-2010} in
\citetitle{rj-ellison-2010} he analyzes how the United states Department of
Defense handles software acquisition in a very high secure environment. In the
context of the military ``[\dots] supply chains typically involve the movement
of materials from home base to troops in theater. The responsibility for
managing these supply chains falls to the acquisition and logistics
experts''~\cite{rj-ellison-2010}. In this case, the DoD is not producing any
software. Their only concern is to get the software developed, tested, shipped
and updated in a secure fashion. In all of these steps they might relay on
outside contractors and thus have to safeguard themselves against any risks.
This in turns makes the supply chain far larger, and far more encompassing then
a development model like the software lifecycle looked upon earlier.

In this thesis we will focus on how package managers work as a supply chain for
distributing software in a secure manner.

\section{Linux Distributions}\label{sec:linux_distributions}
\subsection*{Linux}
Linux is a free and open-source kernel. It was first developed by Linus Torvalds
in the early 1991 and has grown into the largest open-source project
today~\cite{linuxfoundation-linux}. It is commonly used in everything from
firmware modules on a computer, to the ever increasing field of Internet of
Things, along with servers and on personal computers. The development of Linux is
distributed and has spawned the open-source method of developing software.

Linux is accompanied by a suite of tools and environment that is commonly
referred to as a distribution and defines an operating system based on
Linux. These are created by companies as commercial products, as well as groups
of volunteers as a hobby for free. The tooling of these distributions, along
with organization and the inherent supply chain to deliver artifacts to the
users, are unique to each project. Some are ``source''-distributions, and only
distribute build recipes, and some distribute pre-compiled binary packages.

\subsection*{Debian}
Debian was one of the first operating systems based on Linux, and was created by
Ian Murdock in 1993. One of the main innovations from Debian was the creation of
the very first package manager~\cite{debian-history}. Package manager allows
users to download pre-compiled software from centralized repositories maintained
by the Debian developers. This allows users to easily fetch, update and remove
installed packages on their system.

These packages are maintained by package maintainers who package, update and
maintain the required files to distribute the packages to the end user. Each
maintainer has a cryptographic secure signing key they use to fetch, and publish
source packages to a build server. These source packages contains the needed
files, patches and package files to compile the project source code to a format
accepted by the Debian package manager, APT, which allows the users to easily
install and remove software.

The build server verifies the signatures and compile these source packages to
all the supported architectures. It will then sign these packages with its own
key, and distribute them to the end-user in the form of a mirror system.


\section{Software determinism}\label{sec:reproducible_builds}
Supply chains can be complicated and ensuring that all parts of it is secured
can be hard work. How can we make sure the packages from these supply chains are
not tampered with? Modern toolchains that aids in building software can be
complicated and embed a lot of information which might not be present in future
builds. We will be taking a look at some of the fundamental work that has gone
into considering undeterminism in software and also look at the Reproducible
Builds effort which is a software development model to aid in producing
deterministic software.

\subsection*{Trusting trust}
In a paper from Ken Thompson in 1984 (after he won the Turing Award for his work
on the UNIX operating system) \citetitle{Thompson:1984:RTT:358198.358210},
Thompson as a programming exercise implements a very basic self-reproducing
program. As a demonstration, he adds code capable of introducing new code when
certain patterns are encountered. This open up the possibility of having
malicious code inserted into compiled code that could be leveraged by malicious
actors.

\begin{quotation}
``You can't trust code that you did not totally create yourself. [\ldots] No amount
of source-level verification or scrutiny will protect you from using untrusted
code. In demonstrating the possibility of this kind of attack, I picked on the C
compiler. I could have picked on any program-handling program such as an
assembler, a loader, or even hardware microcode. As the level of program gets
lower, these bugs will be harder and harder to detect. A well-installed
microcode bug will be almost impossible to
detect''\cite{Thompson:1984:RTT:358198.358210}.
\end{quotation}

This paints a very bleak picture, considering most software we get today is
pre-compiled and provided to us by different vendors. There is no clear cut of
verifying what is provided by these vendors. But there are possible ways to
counter this problem.  David A. Wheeler in his dissertation
\citetitle{Wheeler:2005:CTT:1106778.1106809} details a possible solution to the
``trusting trust''-problem. It involves what he call ``diverse
double-compilation''~\cite{Wheeler:2005:CTT:1106778.1106809}.

``Diverse double-compilation'', or ``DCC'', is the act of using two compilers to
detect any difference in the resulting artifact. The first compilation is done
with a secondary compiler, then again with the primary compiler. The idea is
that the secondary compiler is a minimal implementation of the compiler, and can
be trusted. However, creating compilers is not a trivial task. If we want to
have a trusted and verified compiler that is capable of outputting the same
binary we need to write these compiler our self. This gets complicated quickly
when you consider the time and effort spent on writing compilers for languages
such as C++. Instead we have made an effort the past years to reproduce
deterministic software in other ways.


\subsection*{Reproducible builds}
Reproducible builds is a set of practices on how to achieve deterministic
compilation of software. Supply chains are usually handled with multiple tools,
and on several individual computers. This leaves a rather large attack surface
for malicious actors to try compromise the chain. This is not a theoretical
threat. There has been an increase in attacks on parts of supply chains in
recent years. They are high impact and affect users as well as developers.

According to the definition of reproducible builds;

\begin{quotation}
``A build is reproducible if given the same source code, build environment
and build instructions, any party can recreate bit-by-bit identical copies of
all specified artifacts''~\cite{reproducible-builds-2019-definitions}.
\end{quotation}

One of the earliest open-source projects to promote reproducible builds is the
Tor project. The Tor project develops the ``Tor network'' which is an anonymity
network compromised of volunteers that run network nodes that effectively
anonymize the network of the user~\cite{tor}. This is heavily used by dissidents
in oppressive regimes where the internet connection is filtered, or partially
blocked. To help giving access to this network, they develop a variant of the
web browser Firefox called ``Tor browser''~\cite{tor-browser}.

The browser is configured to utilize the Tor network for anonymous internet
browsing. Marginalized protesters has been widely using Tor to circumvent
censorship. The supply chain and distribution of this software is important as
protesters and marginalized groups need to make sure the browser is not
compromised. Receiving a malicious version of this software could in many cases
result in prison or life threatening danger to people. Mike Perry highlighted
the concern when discussing the testing of the ``Tor browser''.

\begin{quotation}
    ``[...] software development has to evolve beyond the simple models of
"Trust my gpg-signed apt archive from my trusted build machine", or even
projects like Debian going to end up distributing state-sponsored malware in
short order''~\cite{mike-perry-2013}.
\end{quotation}

The result of this concern is the move to support reproducible builds: Allowing
users, and multiple independent builders, to recreate the distributed artifact
bit-for-bit~\cite{unknown-2014}. This lets the user themselves compile the Tor
browser and make sure there has been no tampering with the distributed binaries
if there was a suspicion they where. This is done by utilizing Gitian which
builds and packages the software on self-contained virtual
machines~\cite{gitian}. This enables the project to distribute the same build
instructions as used to package the software in the first place, and allows
users to easily verify if the distributed artifact matches the self-produced
one.

Around the same time as Gitian was discussed, in 2013, Debian started a push
towards reproducible builds, and an effort into achieving this for their
distributed packages~\cite{debian-repro-2013}. Since then, 22 projects are
officially part of the initiative to bring reproducible builds to users, among
others are Arch Linux, Fedora, Tor, openSUSE and more~\cite{repro-who}. There
has been 4 summits held for volunteers to come together and discuss the issues
at hand~\cite{repro-paris}.

\subsection*{Source Date Epoch}
One of the most common offenders for undeterministic builds is the embedding of
when something was built. On the surface this looks like a completely innocent
thing to do for most builds, but it creates problems when the produced artifact
in turns become undeterministic because we built it at another point in time.

The Reproducible Builds project defines an environment variable called
``SOURCE\_DATE\_EPOCH'' which is a means to solve this dilemma
\cite{reproducible-builds-2019-source-date-epoch}. This variable enables software
distributors to build artifacts with an embedded time, but it also helps to
specify the time in a manner that enables reproducible builds. We can record
this variable, and at a later point in time define the variable to pretend we
build this at the given time.

The requirement is that this variable is exported to the build system used to
create the package. It also needs to take the current date time if no
``SOURCE\_DATE\_EPOCH'' is provided.

\subsection*{Buildinfo}%
\label{sub:buildinfo}
One of the main issues with reproducible builds is that it is hard to make
everything universally reproducible. Producing the same binary package on
multiple different Linux distributions is close to impossible and unmanageable
for most software. Thus we need to specify the environment being utilized with
all of the requirements and idiosyncrasies.

The environment is made up of a few things. The installed software installed on
the system. What shell variables are present as it denotes the expected
timezone, language settings and any other special requirements. These things can
make builds behave differently and are thus important to record and keep track
of.

This is not a new discovery. \citeauthor{software-reconstruction-1999} in their
paper \citetitle{software-reconstruction-1999} from 1999, defined a ``Bill
of Material'', or a ``BOM'' for short.

\begin{quotation}
``Document all of the components that contributed to the build inn a list, i.e.,
a bill of materials (BOM). The BOM may contain the names, versions, and
directory paths of operating systems, libraries, compilers, linkers, make-files,
build scripts, etc The BOM may be manually created, but many configuration
management tools generate it as a byproduct of the build''~\cite{software-reconstruction-1999}.
\end{quotation}

\begin{listing}[H]
\begin{minted}[]{text}
Format: 1.0
Source: dh-make
Binary: dh-make
Architecture: all
Version: 2.201802
Checksums-Sha256:
22c95094efbe79445336007dd[...] 42360 dh-make_2.201802_all.deb
Build-Origin: Debian
Build-Architecture: amd64
Build-Kernel-Version: 4.9.0-8-amd64 #1 SMP Debian 4.9.110-3 (2018-10-08)
Build-Date: Thu, 06 Dec 2018 00:04:23 +0000
Build-Path: /build/dh-make-2.201802
Installed-Build-Depends:
autoconf (= 2.69-11),
automake (= 1:1.16.1-4),
[...]
xz-utils (= 5.2.2-1.3),
zlib1g (= 1:1.2.11.dfsg-1)
Environment:
DEB_BUILD_OPTIONS="buildinfo=+all reproducible=+all parallel=16"
LANG="C"
LC_ALL="POSIX"
SOURCE_DATE_EPOCH="1543231660"
\end{minted}
\caption{Example buildinfo file}
\label{lst:buildinfo}
\end{listing}

Such a file would contain all the requirements to recreate the environment the
artifact was built inn. Because each ecosystem has their own way of parsing
information, there is a need for multiple formats to define the requirements.
Currently there are around 5 different formats for different ecosystems on the
reproducible builds website~\cite{reproducible-builds-2019}.

The Debian format encompasses a wide array of values. ``Format'' defines the
expected fields in the format and gets incremented with any changes.  ``Source''
and ``Binary'' defines the source package used to produce the artifact, and the
corresponding binary package it outputs. The ``Architecture'' fields defines
which architecture the product is compiled towards. Debian supports a wide array
of CPU architectures from ARM to AMD 64 bit. High-level languages usually does
not compile, therefore ``all'' is used to denote the
architecture~\cite{deb-buildinfo}.

The ``Build-'' variables denote the build environment used to create the
artifact in the Debian ecosystem. Since Debian has a slew of derivative and
closely related distributions, ``Build-Origin'' is used to denote this
distribution.  ``Build-Architecture'' denotes the architecture of the build
server being used.  ``Build-Kernel-Version'' denotes the explicit  version of
the kernel used. This is commonly fetched with the command ``uname -a''.
``Build-Date'' refers to the ISO compatible date when the process took place.
``Build-Path'' is the location where the build took place.
``Installed-Build-Depends'' contains list of all packages present during the
packaging of this artefact. This s an important list to keep track of as it
enables the complete recreation of the environment at a later point.

Linux has several variables that set the locale, language and timestamp format
that can affect the build process. ``Environment'' encompasses all of these.
Most importantly the variable ``SOURCE\_DATE\_EPOCH'' is stored here to make
sure timestamps are deterministic.

This file can then be distributed alongside the package, or provided through
other means. Debian archives all buildinfo-files on a centralized webpage where
they can be queried and retrieved~\cite{buildinfo-debian}.

When distributing files it is very common to do so using file archives. Common
formats are the ZIP and the TAR format. The order in which files appear in the
archive needs to be consistent for the checksum of the archive to match, however
this can be hard to test in some cases. ``disorderfs'' is a filesystem that lets
you introduce unexpected and randomized behavior when reading files. This helps
find sources for non-deterministic artifacts in the build process.

Diffoscope is a tool to help compare binary formats for differences. It supports
a lot of binary formats to help find reproducability issues in produced
artifacts. It enables the user to output reports of the comparison as plain text
files or HTML files so they can be easily embedded in webpages. The current CI
system in Debian provides the HTML files for easier debugging.

The software is packaged and used by a number of distributions and is currently
a very important tool in debugging reproducability issues.


\section{Rebuilders}%
\label{sec:rebuilders} 
One of the main ideals with reproducible builds is the ability to let the users
recreate distributed artifacts. This is achievable with the correct tooling, and
a ``BUILDINFO'' file as specified in Section~\ref{sub:buildinfo} on
page~\pageref{sub:buildinfo}. However, building all distributed packages is an
unwieldy task. The appeal of Linux distributions is the ability to download
pre-compiled packages to save the effort of building all the software one
intends to use.

Debian has put a great deal of effort into testing packages by setting up a
``Continuous Integration'' framework for testing packages~\cite{debian-ci}.
This setup compiles all Debian packages twice with variations to see if they end
up reproducible or not. This is a neat approach to find reproducability issues,
but it does not reproduce any packages produced and distributed by Debian
developers directly. They are merely built twice in their own environment with
the packaging files distributed by the developers. The real goal is to reproduce
distributed packages, so the CI solution it self does not fulfill this goal.

What is needed are servers which take the same packages which are distributed,
and reproduces these. The general idea is that there should be a pool of diverse
servers which are capable of rebuilding packages in the correct
environment~\cite{reproducible-builds-sharing}. These rebuilders should be
capable of sharing build attestations of the built package. The build
attestations used in this project is the BUILDINFO files, described in
Section~\ref{sub:buildinfo} on page~\pageref{sub:buildinfo}, and the in-toto
link metadata described in Section~\ref{sub:in-toto} on
page~\pageref{sub:in-toto}. The in-toto link metadata files will be used to
verify the downloaded package on the user system.

\section{Rebuilder logs}%
\label{sec:rebuilder_logs}
The rebuilder system needs to publish build attestation and the results of the
package builds it performs. The files published is the in-toto link metadata
used for package verification, and BUILDINFO files which describes the build
environment. Since these are log files, they could be tampered with and altered
to pretend a malicious package build is in fact correct. What we need is
evidence whether or not the logs has been tampered with. This can be
accomplished by utilizing something called transparency logs. These data
structures are build on Merkle trees, where nodes are hashed together to
construct a binary tree, which can prove whether or not a published log has been
tampered with. In this section we will take a look at how such logs work and the
theory behind them.

\subsection{Merkle Trees}%
\label{sec:merkle_trees}
Merkle Trees is a tree structure based on cryptographic secure hashing function
\cite{ralph-c.-merkle-1998}. It creates a binary tree where each leaf is hashed
together to create interior nodes. The top node of this tree is reffed to as a root
node and the data structure is essentially a binary tree structure where two
nodes has two children. The main idea behind this scheme is to provide a
cryptographic signature that can consist of multiple messages together. But
Merkle tree has most widely been used for creating tamper-evident logs.

\subsubsection*{Transparency logs}%
\label{sub:certificate_transparency_log}
Transparency logs is a use case of Merkle trees by~\citeauthor{182788} which
describes how this data structure can be used to achieve tamper evident
logging~\cite{182788}. In the traditional sense, logs can be plain text files
published anywhere. It can simply be a line of text detailing something. If this
where to change on a remote service, there would not normally be any evidence of
this. What a transparency log attempts to accomplish is to create hashes of the
log entries, and create a tree. If we store the hash of this tree, we could
prove the log has been modified at some point in time after the has we got was
provided. 

Certificate transparency logs
from~\citeauthor{b.-laurie-a.-langley-e.kaster-google-2013} is an implementation
of transparency logs. These logs lets organizations that issue HTTPS
certificates an audit trail~\cite{b.-laurie-a.-langley-e.kaster-google-2013}.
It allows the discovery of when abuse, and creation of malicious certificates.
The log allows organizations to see who issued certificates for what domains,
and can aid in detecting private key compromise, API misuse or other types of
misuse early.

To make sure we can prove evidence of tampering, transparency logs has the
unique feature where we can provide some hashes from the tree, and reconstruct
an assumed root node. Given a sound and secure hashing function, difference in
the root tree checksum proves if modifications has been done on the tree.  The
proofs needed to implement a transparency log is as follows;

\begin{itemize}
\item Audit proof
\item Consistency proof
\end{itemize}

Proofs are tuple pairs where the first element describes the position of the
hash, which can be either left or right, and the second element includes the
hash of the given object. Given the correct order of hashing, the product should
be some Merkle tree root the log is either currently using, or have used in the
past. If this is not the correct root hash it proves some tampering has been
done on the log.

\begin{figure}[H]
\centering
\subfloat[Merkle tree with leaf ``d5'' highlighted]{\begin{tikzpicture}
\Tree
[ .\node[root]{r};
[ .m   [ .i d1 d2 ]  [ .j d3 d4 ]  ]
[ .n  [ .k \node[proof]{d5}; d6 ]  [ .l d7 d8 ]  ]
]
\end{tikzpicture}}%
\qquad
\subfloat[needed nodes for audit proof of ``d5'']{\begin{tikzpicture}
\Tree
[ .\node[root]{r};
[ .\node[roots]{m};   [ .i d1 d2 ]  [ .j d3 d4 ]  ]
[ .n  [ .k \node[proof]{d5}; \node[roots]{d6}; ]  [ .\node[roots]{l}; d7 d8 ]  ]
]
\end{tikzpicture}}%
\caption{Audit proof}
\label{fig:audit}
\end{figure}

The audit proof is used to verify that the given element in the log has not been
tampered with. The tree root is given, along with the elements needed to
recreate the missing nodes for the root. In the Figure~\ref{fig:audit} we can
see the representation of a Merkle tree. The leafs in this tree $ d1,\ d2,\ d3,
\ldots d8 $ are hashes of the log values we are storing on the tree. In our
visualizer components this is the rebuild submissions consisting of the
BUILDINFO file and in-toto link metadata.

For the proof that the node $ d5 $ is part of the log we need to reconstruct the
tree root $ r $. It's important to realize that hashes can be a shortcut when
reconstructing the tree. We only need the tree of the largest subtree the leaf
is not a part of. Since we want to check $ d5 $, the leafs ${d7,\ d8}$ are not
needed, we can just utilize the complete subtree $ l $ which is the hash of both
those nodes. This ensures that we only need the minimal amount of hashes to
reconstruct the tree root $ r $.

To reconstruct the tree root, the tree nodes we need are $ r = \{ d5, d6, l, m
\} $.  If they are hashed together appropriately, such as $ r = H( m, H( H(d5,
d6), l)) $ where $ H $ defines a secure hashing function such as SHA256 or
SHA512, we will arrive at the same value for and thus the proof is validated.

The consistency proof is used to verify that the log is operating as an
append-only log in a correct manner. This proof requires two things. A
previous Merkle tree root, and the number of leaves present at the time of
this tree root. The returned path is the number of subroots needed to
recreate the path from the root, until the new root.

To have a correctly vetted tamper-evident log, there needs to be monitor server.
Monitor servers watch new entries from log servers and verify them. They collect
the current signed tree root, and make sure they match up with the currently
fetched server. They are important to make sure that the log collecting server
do not tamper with the entries on the log and thus do not tamper with the
entries on the log and thus behaves properly.

\subsection*{Transparency Log Overlays}%
\label{sub:transparency_log_overlays}
For our system we will add some semantic meaning to the leaves we add to the
Merkle tree. This will be explained in greater detail in the development of
this in Section~\ref{sub:fourth_iteration_development} on
page~\pageref{sub:fourth_iteration_development}, however we will go into some of
the underlying concepts.

\citeauthor{10.11452976749.2978404} describes
in~\citetitle{10.11452976749.2978404} a concept of adding transparencies on top
of existing processes to achieve transparency and tamper-evident
events~\cite{10.11452976749.2978404}. They create a system greatly inspired by
the work of both~\citeauthor{182788}, in transparency logs~\cite{182788},
and~\citeauthor{b.-laurie-a.-langley-e.kaster-google-2013} in certificate
transparency logs~\cite{b.-laurie-a.-langley-e.kaster-google-2013}, and builds
on top of of the concepts describe in the paper.

The idea of this approach is to log the everyday events of systems on a
transparency log to make sure the events are tamper-evident. The values of these
commitments on the log can be arbitrary and aid in providing some semantic
meaning to the underlying application. In our project, we will utilize this idea
to provide rebuild attestations, but also the ability to revoke such attestations
on the Merkle tree.

\section{Research Overview}%
\label{sec:research_overview}
In this section we will be taking a look at the work done in the surrounding
areas when it comes to reproducible builds, supply chain security and
transparency logs.

\subsection{Software Distribution Transparency and Auditability}\label{sec:benjamin}
The initial work on package transparency logs, as a form of binary transparency,
was done by \citeauthor{1711.07278v1}. It implements an append-only Merkle tree to
keep track of released versions of software into a package repository, namely
Debian~\cite{1711.07278v1}.

The paper details a novel hidden attack, where a backdoored package is
distributed to some users but not everyone. As the transparency log knows which
package is the correct one, it is possible to detect such attacks. The
implementation also defines log monitors, that peek at inclusions and make
sure the logs are operating properly and doesn't leave out information.

The usage of transparency logs in this project inspired the work done in this
thesis. The idea here is that the transparency log by~\citeauthor{1711.07278v1}
can be applied as release monitor and detect malicious or compromised signing
keys by developers. This is an addition to the rebuilder system described in
this thesis, and the goal is to see if the addition of transparency logs on the
rebuilder gives us any security guarantees.


\subsection{CHAINIAC}\label{sec:chainiac}
CHAINIAC by \citeauthor*{kirill-niktin-2017} introduces a framework to help
collectively validate source-to-binary correspondence\cite{kirill-niktin-2017}.
This is done by introducing a cothority.  Each developer commits their binaries
and corresponding source code to a Merkle tree which is then signed by the
developers. This is built through a distributed system which will rebuild and
corroborate their results to aggregate signatures on packages which makes up the
cothority. The number of required signatures from this cothority is defined by
the project and pushed to an updated timeline.

The main problem with this project is that it imposes new requirements on the
distribution developers. Changing how distribution developers work with
packages, and how packages are released is something that has been done for
years. Changing this approach is not going to help adoption by the distribution.
The rebuilder system in this thesis is an extension of the current release
process, it does not impose any new methods or requirements on the developers.

\subsection{Contour}\label{sec:contour}
Contour by \citeauthor{1712.08427v2} is a system that implements binary
transparency by utilizing blockchains. In blockchains Merkle trees are used to
store the data. In this implementation the resulting binaries are hashes on the
chain. The resulting Merkle root is then hashed with previous roots to provide
the transaction and a block header. This is again distributed and secured from
split view attacks, where one server presents the client with a malicious view
of the world. Split view attacks enable the remote log to provide proofs to the
client which are not present elsewhere. The solution to this is by adding a
consensus on top of the tree. The implementation was tested using the Python
packaging index, PyPI, and the Debian package repository to test the
solution~\cite{1712.08427v2}.

This project attempts to commit the Debian package index to a Bitcoin blockchain
to make sure the index stays consistent across the ledger. This is an
integration which is strictly not needed to verify packages. Our research
project presents a simpler solution where consistency across rebuilders are not
needed.

\subsection{Go transparency log}\label{sec:go-transparency-log}
Go has been developing their dependency management the past year. With the
release of their new dependency manager, go mod, they now produce dependency
information along with a lockfile. The lockfile, go.sum, contains packages along
with versions and checksums.

A recent proposal by \citeauthor{russ-cos-and-filippo-valsorda} details how this
go.sum file would be commited to a transparency log to provide a verification
method for dependency releases in the go ecosystem. It implements a own database
to model Merkle trees, and lets nodes validate and monitor the tree for
dependency inclusions. The idea is to check this log server when dependency
files are downloaded for proofs of releases.\cite{russ-cos-and-filippo-valsorda}

This project only appends the project dependencies as a way to pin it, and
enables developers to make sure they have received the proper dependency listing
of the project.


\section*{Summary}\label{sec:summary-theory} 
In this chapter we have taken a deeper look at the theory surrounding software
distributions, supply chains, reproducible builds and Merkle trees. The supply
chain is important to secure, and Reproducible Builds is possibly one way to
achieve deterministic building in the world of Linux distributions.

We have looked at how rebuilders can help verify packages are reproducible and
have introduced transparency log as a way to make sure the logging of these
package builds can not be tampered with after they have been published.

In the next chapter we will be taking a look at the technologies we will be
utilizing for this project.

\end{document}
