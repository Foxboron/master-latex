% !TEX encoding = UTF-8 Unicode
%!TEX root = ../Main/thesis.tex
% !TEX spellcheck = en-US
%%=========================================
\documentclass[../Main/thesis.tex]{subfiles}
\begin{document}
\chapter{Evaluation}%
\label{ch:evaluation}
In this section we will be evaluating the project. We will be utilizing the
outline described in Chapter~\ref{ch:methodology} to help evaluate the final
research artifacts we are left with.

So far we have had four iteration on this project. First iteration was to
reimplement the API and functionality from the first visualizer, the second
iteration implemented support for merkle trees, the third iteration implemented
the needed signing layer on the tree, and the fourth and final iteration that
implements the transparency log overlay.

The APIs we created during the different iterations of this project is what we
will be taking a look at first. For this evaluation the attributes for modular
API design as outlined by~\citeauthor{Iyer:2012:EAC:2342209.2342213} is going to
be used~\cite{Iyer:2012:EAC:2342209.2342213}. We will then be doing a technical
evaluation of the transparency log implementation by stress testing the data
storage and comparing it to the expected amount of data from published Debian
packages. The APT integration is going to be evaluated last so we can consider
some of the possible security guarantees.

Appendix~\ref{appendix:evaluation} on page~\pageref{appendix:evaluation} is
going to contain any auxiliary details  and information for this evaluation.

% \section{APT Integration and Security}%
% \label{sec:apt_integration_and_security}

\section{API Evaluation}%
\label{sec:api_evaluation}
The APIs we are going to evaluate are the transparency log API from
Table~\ref{api:transparency_log} on page~\pageref{api:transparency_log}, the
crypto API from Table~\ref{api:crypto_api} on page~\pageref{api:crypto_api} and
the transparency overlay API from Table~\ref{api:Overlay API} on
page~\pageref{api:Overlay API}.

We are not going to consider~\ref{api:old_visualizer} on
page~\pageref{api:old_visualizer} as it predates this thesis and is mainly
reimplemented for the sake of compatibility with the current rebuilder setup.

\subsection*{Functionality}%
\label{sub:functionality}
This attribute centers around the need to make functionality offered intuitive.
The developed APIs centers on the concept of standardized REST API concepts and
should be easy for anyone familiar to utilize. The API also covers all the
requirements to interact with the transparency log provided.

\subsection*{Hierarchy}%
\label{sub:hierarchy}
When designing complex API there is a chance internal implementation detail
leaks to the user of the API. In our current implementation internal details are
exposed as we have incrementally developed our API. We are capable of adding
pure JSON objects to our log, as well as adhere to the entry definitions for the
overlay.

\subsection*{Separation of concerns}%
\label{sub:seperation_of_concerns}
Separation of concerns in this scenario is handled well. We have split
functionality across multiple APIs with clearly labeled names. The tree API does
not implement functionality in the merkle tree API. However, when utilizing the
API one has to use APIs from both the transparency log API and the transparency
overlay API.

\subsection*{Interopability}%
\label{sub:interopability}
Interopability is how easy it is to interact with this API. Complicated APIs
might be less useful as the tools available might be sparse.  The way to
interact with this API is through REST API calls. This is widely used and a lot
of tools are available to interact with APIs designed in this fashion.

\subsection*{Reusability}%
\label{sub:reusability}
The API is not designed to be inherently reusable for other usages than the one
it is supposed to cover. We can however use the raw merkle tree without
utilizing the more abstracted API calls in this implementation.

\section{Transparency log testing}%
\label{sec:transparency_log_testing}
To test the Merkle tree implementation we need to first consider the amount of
data this system would realistically receive. We have implemented this tree on
top of a SQL database, so performance impacts should be checked to see how large
a tree can grow before any noticeable impacts can be found.

Debian builds and releases packages into a testing repository on a daily basis,
this produces the BUILDINFO files we will be using to recreate packages in the
rebuilder. This can be retrieved from a centralized server with an API. However,
since the API was tedious to work with there has been put up a file server with
all the BUILDINFO submissions. This allows us to grab the files produced for
each day of the year.

The file server contains builds information from 2016 until 2019 and totals an
archive of around 130 GB. As the merkle tree grows for every insertion and
hashes interior nodes along with signing the root node, we need to limit the
data for practical reasons. We are going to only consider the build submissions
from 1st of January 2019, until 19th of May 2019 (see
Appendix~\ref{appendix:buildinfo_ftp_server} on
page~\pageref{appendix:buildinfo_ftp_server}).

\begin{table}[htpb]
\centering
% \setlength\extrarowheight{2pt}
\begin{tabulary}{1.0\textwidth}{|l|L|}
\hline
    \textbf{Date} & 
    \textbf{Count} \\
\hline
    2019-01 & 41566 \\ \hline
    2019-02 & 29750 \\ \hline
    2019-03 & 18354 \\ \hline
    2019-04 & 10061 \\ \hline
    2019-05 &  5288 \\ \hline
    \textbf{Total} & 105019 \\ \hline
\end{tabulary}
\caption{Debian package builds from 1st of January until 19th of May}
\label{evaluation:buildinfos_ftp}
\end{table}

Table~\ref{evaluation:buildinfos_ftp} shows the number of builds done for each
month. There is a slight spike in January with a subsequent falloff through the
spring. The total submissions is around 105000. The goal of the testing is to
see how well the current implementation can handle around 105000 elements on the
tree. What is the performance, and is it acceptable for implementation in the
real-world in its current state?
 
\subsection*{Testing setup}%
\label{sub:testing_setup}
One of the goals as outlined in the introduction is the ability to reproduce the
results of the thesis. For this we need to have a way to easily create the
components needed for the testing. For the transparency log we need two
components, one database service running PostgreSQL, and one service to run the
visualizer component. To make sure this setup is easy for future researcher to
set up, we utilize a technology called containers. Containers are light weight
process separations on the host operating system. The commonly used tool for
creating containers is called Docker, as described
in Section~\ref{sub:docker_and_containers}.

Docker allows us to define the components in a way that lets us easily build and
tear down the systems without having to manually setup the services themselves.
The testing is done with this setup to make sure reproducible results can be
achieved. We also provide a ``Makefile'' which is a standardized format for
executing build, testing and installation scripts for software. This allows us
to provide a generic interface where one can run the test setup in the future.

The test data we are using is small key value pairs with an incremented number.
The reason for this is because creating and faking the appropriate data would
require a fair bit of parsing of the BUILDINFO files as link metadata is not
included in the file server.

See Appendix~\ref{appendix:merkle_tree_stress_test} on
page~\pageref{appendix:merkle_tree_stress_test} for the testing instructions.

\subsection*{Testing results}%
\label{sub:testing_results}
For the testing of the tree, there was around 113000 elements appended to the
transparency log. The intentions is to test what would happen with the data
storage when approaching an expected amount of build submissions on the log.
As the transparency log is going to be used by the APT client to find the
metadata needed to verify packages we need to see if the log interaction is
going to slow down the process.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{../Diagrams/evaluation/response_time.pdf}
\caption{Response time on entry inclusion}
\label{fig:response_time}
\end{figure}

Figure~\ref{fig:response_time} shows the growth in response time when appending
new entries to the log. The log has a blue line, which is the response time, and
a orange line which is the mean response time as we move through the graph. The
minimum response time was below 0.1 seconds, and the maximum response time
observed was 12 seconds. The testing itself lasted around 120 hours, or around 5
days. The size of the table in complete ended at around 243 MB. Consistency and
audit proofs also spend approximately the same time for each query.

12 second is not by any means a good metric in this case. We need to do multiple
consistency proofs and audit proofs for each package we are going to install.
This quickly adds up in seconds and slows down the package installation process
by a non trivial amount of time. The main reason after some investigation is
that relationship building is taking a fair amount of time.

PostgreSQL is not well optimized for joins and relations when it's done on the
same table. This quickly adds up seconds when we do several queries to get the
object models from the ORM in place.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{../Diagrams/evaluation/nodes.pdf}
\caption{Number of nodes over time}
\label{fig:nodes_figure}
\end{figure}

Figure~\ref{fig:nodes_figure} shows the number of leafs and interior nodes that
get created over time. For 113054 leafs a total number of 819533 interior nodes
get created. This amounts to a complete 932587 objects on the database table.
This suggests a naive implementation in the append function we implemented
in Section~\ref{sub:second_iteration_development} on
page~\pageref{sub:second_iteration_development} In a proper implementation the
number of interior nodes should mirror the number of leafs. This is the issue
causing the poor performance of the tree.

However, the proofs works and has been correctly implemented as proven when we
implemented it into the APT transport in Section~\ref{sub:fourth_iteration_results} on
page~\pageref{sub:fourth_iteration_results}. The poor performance does not
affect the correctness of the tree.

\section{Summary}%
\label{sec:evaluation_summary}
In this chapter we have taken a look at the implementation of the new visualizer
component for the rebuilder system. We have assessed how well the implementation
written in this thesis can handle real-world data. The data we compared to was
the number of package builds Debian have done over a 5 months period. 

The assessment shows that the current implementation does not scale very well
considering the amount of data it would be required to store. There needs to be
more research into how this implementation can be improved.

\blankpage
\end{document}
